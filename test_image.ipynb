{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import os\n",
    "# import time\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# def plot_digit_arr(img_data):\n",
    "#     shape = img_data.shape[0]\n",
    "#     size = int(shape**0.5)\n",
    "#     img = img_data.reshape(size, size)\n",
    "#     plt.imshow(img, cmap=\"binary\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "# def split_image_digits(path):\n",
    "#     img = cv2.imread(path)\n",
    "#     img1 = img[0:44, 0:92]\n",
    "#     img2 = img[0:44, 92:184]\n",
    "#     img3 = img[0:44, 184:276]\n",
    "#     # img1 = img1.reshape(-1, 4048).astype(np.int8)\n",
    "#     # img2 = img2.reshape(-1, 4048).astype(np.int8)\n",
    "#     # img3 = img3.reshape(-1, 4048).astype(np.int8)\n",
    "#     return img1, img2, img3\n",
    "\n",
    "# def crop_images(paths):\n",
    "#     i = 0\n",
    "#     img_dir = os.path.join(os.getcwd(), \"datasets\", \"images\")\n",
    "#     if not os.path.exists(img_dir):\n",
    "#         os.mkdir(img_dir)\n",
    "#     for path in paths:\n",
    "#         i += 1\n",
    "#         new_img_path = os.path.join(img_dir, \"img{}.png\".format(i))\n",
    "#         img_data = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "#         img = img_data[0:45, 0:278]\n",
    "#         cv2.imwrite(new_img_path, img)\n",
    "\n",
    "# def create_image_data(img_dirname, outdirname=\"newer_images\"):\n",
    "#     imgs = []\n",
    "#     images_dir = os.path.join(os.getcwd(), \"datasets\", img_dirname)\n",
    "#     outdir = os.path.join(os.getcwd(), \"datasets\", outdirname)\n",
    "#     if not os.path.exists(outdir):\n",
    "#         os.mkdir(outdir)\n",
    "#     paths = [os.path.join(images_dir, p) for p in os.listdir(images_dir)]\n",
    "#     crop_images(paths)\n",
    "#     i = 0\n",
    "#     for path in paths:\n",
    "#         img1, img2, img3 = split_image_digits(path)\n",
    "#         path1 = os.path.join(outdir, \"image{}.png\".format(i+1))\n",
    "#         path2 = os.path.join(outdir, \"image{}.png\".format(i+2))\n",
    "#         path3 = os.path.join(outdir, \"image{}.png\".format(i+3))\n",
    "#         cv2.imwrite(path1, img1)\n",
    "#         cv2.imwrite(path2, img2)\n",
    "#         cv2.imwrite(path3, img3)\n",
    "#         imgs.append(img1)\n",
    "#         imgs.append(img2)\n",
    "#         imgs.append(img3)\n",
    "#         i += 3\n",
    "#     return imgs\n",
    "\n",
    "# def imgs_to_dict(dirname):\n",
    "#     img_dict = {}\n",
    "#     image_dir = os.path.join(os.getcwd(), \"datasets\", dirname)\n",
    "#     i = 0\n",
    "#     for fname in os.listdir(image_dir):\n",
    "#         img_path = os.path.join(image_dir, fname)\n",
    "#         img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "#         img_dict[\"image{}\".format(i+1)] = img\n",
    "#         i += 1\n",
    "#     return img_dict\n",
    "\n",
    "# imgs = create_image_data(\"images\")\n",
    "# print(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "def normalize_img(img):\n",
    "    img = img * 1.0/255\n",
    "    return img\n",
    "\n",
    "def imgs_to_dict(image_dir):\n",
    "    img_dict = {}\n",
    "    for fname in os.listdir(image_dir):\n",
    "        img_path = os.path.join(image_dir, fname)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        name = fname.split(\".\")[0]\n",
    "        idx = name.split(\"image\")[-1]\n",
    "        img_dict[idx] = img\n",
    "    return img_dict\n",
    "\n",
    "def images_to_arr(obj):\n",
    "    imgs = []\n",
    "    if isinstance(obj, dict):\n",
    "        imgs = [normalize_img(img) for img in obj.values()]\n",
    "    elif isinstance(obj, str):\n",
    "        dir_path = None \n",
    "        if os.path.isdir(imgs):\n",
    "            dir_path = imgs\n",
    "        else:\n",
    "            dir_path = os.path.join(os.getcwd(), obj)\n",
    "        for fname in sorted(os.listdir(dir_path)):\n",
    "            img_path = os.path.join(dir_path, fname)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            img = normalize_img(img)\n",
    "            imgs.append(img)\n",
    "    else:\n",
    "        raise TypeError(f\"type {type(obj)} is not supported\")\n",
    "    return np.array(imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 92)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20177bf3bb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "image_dir = os.path.join(os.getcwd(), \"integer_images\")\n",
    "img1_path = os.path.join(image_dir, \"image40.png\")\n",
    "img1 = cv2.imread(img1_path)\n",
    "img1 = cv2.cvtColor(img1,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "print(img1.shape)\n",
    "\n",
    "plt.imshow(img1, cmap=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['81', '27', '42', '83', '88', '85', '89', '45', '123', '49', '61', '28', '35', '121', '72', '39', '63', '67', '78', '113', '66', '38', '79', '76', '87', '48', '120', '33', '60', '70', '69', '119', '77', '30', '73', '31', '75', '112', '53', '34', '74', '122', '36', '90', '57', '32', '40', '52', '29', '129', '80']\n",
      "(30, 44, 92)\n",
      "(30,)\n",
      "(10, 44, 92)\n",
      "(10,)\n",
      "(10, 44, 92)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "dirpath = os.path.join(os.getcwd(), \"integer_images\")\n",
    "\n",
    "d = imgs_to_dict(dirpath)\n",
    "a = images_to_arr(d)\n",
    "targets = np.array([int(i) for i in list(d.keys())])\n",
    "labels = [i for i in list(d.keys())]\n",
    "labels = list(set(labels))\n",
    "print(labels)\n",
    "label_file = os.path.join(os.getcwd(), \"labels.txt\")\n",
    "# with open(label_file, \"w\") as fh:\n",
    "#     for label in reversed(sorted(labels)):\n",
    "#         fh.write(str(label))\n",
    "#         fh.write(\"\\n\")\n",
    "\n",
    "x_train, x_test = a[:30], a[30:40]\n",
    "y_train, y_test = np.array(targets[:30]), np.array(targets[30:40]) \n",
    "x_val, y_val = np.array(a[40:50]), np.array(targets[40:50])\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "1/1 [==============================] - 1s 533ms/step - loss: 4.8733 - accuracy: 0.0000e+00 - val_loss: 4.8996 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/60\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 4.7572 - accuracy: 0.0000e+00 - val_loss: 4.9563 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/60\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4.6027 - accuracy: 0.0333 - val_loss: 5.1184 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/60\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 4.4137 - accuracy: 0.0333 - val_loss: 5.4719 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/60\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4.2387 - accuracy: 0.0333 - val_loss: 6.0733 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/60\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4.1109 - accuracy: 0.0667 - val_loss: 6.6608 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/60\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.9913 - accuracy: 0.0667 - val_loss: 7.0102 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/60\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.8549 - accuracy: 0.1000 - val_loss: 7.2605 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/60\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.7367 - accuracy: 0.1000 - val_loss: 7.4457 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/60\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.6328 - accuracy: 0.1000 - val_loss: 7.5833 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/60\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.5145 - accuracy: 0.2000 - val_loss: 7.7493 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/60\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.3891 - accuracy: 0.1667 - val_loss: 7.9598 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/60\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.2694 - accuracy: 0.1333 - val_loss: 8.2620 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/60\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.1434 - accuracy: 0.2000 - val_loss: 8.6904 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/60\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.0176 - accuracy: 0.3333 - val_loss: 9.2732 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/60\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8985 - accuracy: 0.3667 - val_loss: 10.0015 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/60\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.7732 - accuracy: 0.3000 - val_loss: 10.8796 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/60\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6503 - accuracy: 0.3333 - val_loss: 11.8560 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/60\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.5307 - accuracy: 0.4333 - val_loss: 12.9026 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/60\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.4201 - accuracy: 0.3667 - val_loss: 13.9847 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/60\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.3036 - accuracy: 0.3333 - val_loss: 15.1925 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/60\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.1682 - accuracy: 0.4667 - val_loss: 16.5991 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/60\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.0405 - accuracy: 0.5333 - val_loss: 18.1725 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/60\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.9180 - accuracy: 0.7000 - val_loss: 19.9598 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/60\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.8051 - accuracy: 0.7000 - val_loss: 21.7802 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/60\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.6964 - accuracy: 0.6667 - val_loss: 23.6649 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/60\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.5815 - accuracy: 0.6333 - val_loss: 25.8518 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/60\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.4487 - accuracy: 0.7667 - val_loss: 28.3820 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/60\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.3267 - accuracy: 0.8333 - val_loss: 30.7934 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/60\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.2192 - accuracy: 0.7667 - val_loss: 33.4306 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/60\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.1333 - accuracy: 0.7667 - val_loss: 35.8483 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/60\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.0657 - accuracy: 0.7333 - val_loss: 39.2197 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/60\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.0602 - accuracy: 0.7000 - val_loss: 40.2871 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/60\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.0065 - accuracy: 0.7333 - val_loss: 41.9452 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/60\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.0129 - accuracy: 0.7000 - val_loss: 45.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/60\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.8995 - accuracy: 0.7000 - val_loss: 48.2578 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/60\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.8380 - accuracy: 0.8333 - val_loss: 49.3491 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/60\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.7560 - accuracy: 0.7333 - val_loss: 50.7529 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/60\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.7148 - accuracy: 0.8333 - val_loss: 52.9487 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/60\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.6597 - accuracy: 0.8667 - val_loss: 54.1280 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/60\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.6275 - accuracy: 0.8000 - val_loss: 54.9616 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/60\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.5666 - accuracy: 0.8333 - val_loss: 56.2283 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/60\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5244 - accuracy: 0.8667 - val_loss: 57.7876 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/60\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.5342 - accuracy: 0.8333 - val_loss: 58.5447 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/60\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4481 - accuracy: 0.9333 - val_loss: 59.8433 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/60\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.4373 - accuracy: 0.9333 - val_loss: 61.6779 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/60\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.3839 - accuracy: 0.9667 - val_loss: 63.4799 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/60\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3868 - accuracy: 0.9667 - val_loss: 64.0781 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/60\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.3334 - accuracy: 0.9667 - val_loss: 64.8475 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/60\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3169 - accuracy: 0.9667 - val_loss: 66.3053 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/60\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.3135 - accuracy: 0.9667 - val_loss: 68.0694 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/60\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.2817 - accuracy: 0.9667 - val_loss: 69.1916 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/60\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.2507 - accuracy: 0.9667 - val_loss: 70.5305 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/60\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2439 - accuracy: 0.9667 - val_loss: 71.3751 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/60\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2410 - accuracy: 0.9667 - val_loss: 72.5708 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/60\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.2213 - accuracy: 0.9667 - val_loss: 73.9403 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/60\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1974 - accuracy: 0.9667 - val_loss: 75.3242 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/60\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1966 - accuracy: 0.9667 - val_loss: 76.2144 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/60\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.1956 - accuracy: 0.9667 - val_loss: 77.8350 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/60\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1948 - accuracy: 0.9667 - val_loss: 78.1006 - val_accuracy: 0.0000e+00\n",
      "Loss Average: 1.7561520581444106\n",
      "Accuracy Average: 0.6016666604205966\n",
      "Loss: 0.19484813511371613\n",
      "Accuracy: 0.9666666388511658\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\", input_shape=(44, 92, 1)),\n",
    "    tf.keras.layers.RandomRotation(0.1),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "  ]\n",
    ")\n",
    "\n",
    "def build_model(input_shape, n_outputs):\n",
    "    model = tf.keras.Sequential([\n",
    "        #data_augmentation,\n",
    "        tf.keras.layers.Reshape(target_shape=(input_shape[0], input_shape[1], 1)),\n",
    "        tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation=tf.nn.relu),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=tf.nn.relu),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        #tf.keras.layers.Dropout(0.25),\n",
    "        tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(n_outputs, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                loss=\"sparse_categorical_crossentropy\",\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# model = keras.Sequential([\n",
    "#     keras.layers.Flatten(input_shape=(44, 92)),\n",
    "#     keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "\n",
    "# # Optional: You can replace the dense layer above with the convolution layers below to get higher accuracy.\n",
    "#     # keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "#     # keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation=tf.nn.relu),\n",
    "#     # keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=tf.nn.relu),\n",
    "#     # keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "#     # keras.layers.Dropout(0.25),\n",
    "#     # keras.layers.Flatten(input_shape=(28, 28)),\n",
    "#     # keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "#     # keras.layers.Dropout(0.5),\n",
    "\n",
    "#     keras.layers.Dense(200)\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#               metrics=['accuracy'])\n",
    "model = build_model((44, 92), 130)\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=60)\n",
    "# model.evaluate(x_val, y_val)\n",
    "scores = history.history\n",
    "loss = np.average(scores[\"loss\"])\n",
    "accuracy = np.average(scores[\"accuracy\"])\n",
    "print(\"Loss Average: {}\".format(loss))\n",
    "print(\"Accuracy Average: {}\".format(accuracy))\n",
    "print(\"Loss: {}\".format(scores[\"loss\"][-1]))\n",
    "print(\"Accuracy: {}\".format(scores[\"accuracy\"][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from pycoral.utils import edgetpu\n",
    "from pycoral.utils import dataset\n",
    "from pycoral.adapters import common\n",
    "from pycoral.adapters import classify\n",
    "from PIL import Image\n",
    "\n",
    "# Specify the TensorFlow model, labels, and image\n",
    "script_dir = os.path.join(os.getcwd(),)\n",
    "model_file = os.path.join(script_dir, 'model_quantized.tflite')\n",
    "label_file = os.path.join(script_dir, 'labels.txt')\n",
    "image_file = os.path.join(script_dir, 'image112.png')\n",
    "\n",
    "# Initialize the TF interpreter\n",
    "interpreter = edgetpu.make_interpreter(model_file)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Resize the image\n",
    "size = common.input_size(interpreter)\n",
    "image = Image.open(image_file).convert('RGB').resize(size, Image.ANTIALIAS)\n",
    "\n",
    "# Run an inference\n",
    "common.set_input(interpreter, image)\n",
    "interpreter.invoke()\n",
    "classes = classify.get_classes(interpreter, top_k=1)\n",
    "\n",
    "# Print the result\n",
    "labels = dataset.read_label_file(label_file)\n",
    "for c in classes:\n",
    "  print('%s: %.5f' % (labels.get(c.id, c.id), c.score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A generator that provides a representative dataset\n",
    "def representative_data_gen():\n",
    "  dirname = os.path.join(os.getcwd(), \"integer_images\")\n",
    "  dataset_list = tf.data.Dataset.list_files(dirname + '/*/*')\n",
    "  for i in range(100):\n",
    "    image = next(iter(dataset_list))\n",
    "    image = tf.io.read_file(image)\n",
    "    image = tf.io.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [44, 92])\n",
    "    image = tf.cast(image / 255., tf.float32)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    yield [image]\n",
    "\n",
    "saved_keras_model = 'models\\\\model.h5'\n",
    "model.save(saved_keras_model)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(saved_keras_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# This ensures that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# These set the input and output tensors to uint8\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "# And this sets the representative dataset so we can quantize the activations\n",
    "converter.representative_dataset = representative_data_gen\n",
    "tflite_model = converter.convert()\n",
    "model_file = os.path.join(os.getcwd(), \"models\", \"model.tflite\")\n",
    "with open(model_file, 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2000)\n",
      "(50, 100, 20, 20)\n",
      "\n",
      "\n",
      "(2500, 400)\n",
      "(2500, 400)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import os\n",
    " \n",
    "img_path = os.path.join(os.getcwd(), \"datasets\", \"digits\", \"digits.png\")\n",
    "img = cv.imread(img_path)\n",
    "gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    "cells = [np.hsplit(row,100) for row in np.vsplit(gray,50)]\n",
    "x = np.array(cells)\n",
    "train = x[:,:50].reshape(-1,400).astype(np.float32) # Size = (2500,400)\n",
    "test = x[:,50:100].reshape(-1,400).astype(np.float32) # Size = (2500,400)\n",
    "print(gray.shape)\n",
    "print(x.shape)\n",
    "print(\"\\n\")\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import os\n",
    " \n",
    "img_path = os.path.join(os.getcwd(), \"datasets\", \"digits\", \"digits.png\")\n",
    "img = cv.imread(img_path)\n",
    "gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    " \n",
    "# Now we split the image to 5000 cells, each 20x20 size\n",
    "cells = [np.hsplit(row,100) for row in np.vsplit(gray,50)]\n",
    " \n",
    "# Make it into a Numpy array: its size will be (50,100,20,20)\n",
    "x = np.array(cells)\n",
    " \n",
    "# Now we prepare the training data and test data\n",
    "train = x[:,:50].reshape(-1,400).astype(np.float32) # Size = (2500,400)\n",
    "test = x[:,50:100].reshape(-1,400).astype(np.float32) # Size = (2500,400)\n",
    " \n",
    "# Create labels for train and test data\n",
    "k = np.arange(10)\n",
    "train_labels = np.repeat(k,250)[:,np.newaxis]\n",
    "test_labels = train_labels.copy()\n",
    " \n",
    "# Initiate kNN, train it on the training data, then test it with the test data with k=1\n",
    "knn = cv.ml.KNearest_create()\n",
    "knn.train(train, cv.ml.ROW_SAMPLE, train_labels)\n",
    "ret,result,neighbours,dist = knn.findNearest(test,k=5)\n",
    " \n",
    "# Now we check the accuracy of classification\n",
    "# For that, compare the result with test_labels and check which are wrong\n",
    "matches = result==test_labels\n",
    "correct = np.count_nonzero(matches)\n",
    "accuracy = correct*100.0/result.size\n",
    "print( accuracy )\n",
    "\n",
    "# Save the data\n",
    "np.savez('knn_data.npz',train=train, train_labels=train_labels)\n",
    " \n",
    "# Now load the data\n",
    "with np.load('knn_data.npz') as data:\n",
    "    print( data.files )\n",
    "    train = data['train']\n",
    "    train_labels = data['train_labels']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
